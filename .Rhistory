#install.packages("car")
#install.packages("readr")
#install.packages("glmulti")
#install.packages("rJava")
#install.packages("lmtest")
#install.packages("cowplot")
library(ggplot2)
library(car)
library(readr)
library(glmulti)
library(lmtest)
library(dplyr)
#input file path
input_R_v2 <- read_csv("/Users/le/Documents/GitHub/CodeSwitchingResearch/11152019_two_entropies_appended_input_R_v4.csv",
col_types = cols(sent_type = col_factor(levels = c("non-code-switch", "code-switch")),
sent_id = col_integer(),
university = col_factor(levels = c("CMU","PIT", "PSU")),
#Categorical
#location encodings
`10_80_10_percent_location` = col_factor(levels = c("1", "2", "3")),
`25_50_25_percent_location` = col_factor(levels = c("1", "2", "3")),
`30_40_30_percent_location` = col_factor(levels = c("1", "2", "3")),
first_middle_last_location = col_factor(levels = c("1", "2", "3")),
#pos_tag_first_cs_word_trans=col_factor(),#introduced later
#deprel_first_cs_word_trans=col_factor(),#introduced later
#Boolean
if_it_is_root = col_logical(),
if_previous_word_is_punctuation = col_logical(),
#Numerical
word_id = col_integer(),
frequency_negative_ln_first_cs_word_trans = col_double(),
surprisal_first_cs_word_trans=col_double(),
average_surprisal = col_double(),
surprisal_of_previous_word=col_double(),
#frequencies obtained with OUR corpus only, we don't report results with them because of the limited size of our corpus, but they seem interesting
bilingual_corpus_frequency_negative_log_first_cs_word=col_double(),
bilingual_corpus_frequency_negative_log_first_cs_word_trans=col_double(),
entropy_at_cs_point = col_double(),
entropy_one_word_after_cs_point= col_double(),
length_first_cs_word_form = col_integer(),
length_first_cs_word_trans = col_integer(),
dependency_distance = col_integer(),
translation_sentence_length = col_integer()
)
, trim_ws = FALSE)
rm(input_R_v2)
#input file path
input_R_v2 <- read_csv("/Users/le/Documents/GitHub/CodeSwitchingResearch/11152019_two_entropies_appended_input_R_v4.csv",
col_types = cols(sent_type = col_factor(levels = c("non-code-switch", "code-switch")),
sent_id = col_integer(),
university = col_factor(levels = c("CMU","PIT", "PSU")),
#Categorical
#location encodings
`10_80_10_percent_location` = col_factor(levels = c("1", "2", "3")),
`25_50_25_percent_location` = col_factor(levels = c("1", "2", "3")),
`30_40_30_percent_location` = col_factor(levels = c("1", "2", "3")),
first_middle_last_location = col_factor(levels = c("1", "2", "3")),
#pos_tag_first_cs_word_trans=col_factor(),#introduced later
#deprel_first_cs_word_trans=col_factor(),#introduced later
#Boolean
if_it_is_root = col_logical(),
if_previous_word_is_punctuation = col_logical(),
#Numerical
word_id = col_integer(),
frequency_negative_ln_first_cs_word_trans = col_double(),
surprisal_first_cs_word_trans=col_double(),
average_surprisal = col_double(),
surprisal_of_previous_word=col_double(),
#frequencies obtained with OUR corpus only, we don't report results with them because of the limited size of our corpus, but they seem interesting
bilingual_corpus_frequency_negative_log_first_cs_word=col_double(),
bilingual_corpus_frequency_negative_log_first_cs_word_trans=col_double(),
entropy_at_cs_point = col_double(),
entropy_one_word_after_cs_point= col_double(),
length_first_cs_word_form = col_integer(),
length_first_cs_word_trans = col_integer(),
dependency_distance = col_integer(),
translation_sentence_length = col_integer()
)
, trim_ws = FALSE)
View(input_R_v2)
head(input_R_v2)
str(input_R_v2)
#count rows of data frame
nrow(input_R_v2)
nrow(input_R_v2[which(input_R_v2$sent_type=="code-switch"),])
nrow(input_R_v2[which(input_R_v2$sent_type=="non-code-switch"),])
plot(input_R_v2$sent_type,input_R_v2$length_first_cs_word_trans)
plot(input_R_v2$sent_type,input_R_v2$translation_sentence_length)
plot(input_R_v2$sent_type,input_R_v2$dependency_distance)
plot(input_R_v2$sent_type,input_R_v2$surprisal_first_cs_word_trans)
plot(input_R_v2$sent_type,input_R_v2$average_surprisal)
plot(input_R_v2$sent_type,input_R_v2$surprisal_of_previous_word)
plot(input_R_v2$sent_type,input_R_v2$entropy_at_cs_point)
plot(input_R_v2$sent_type,input_R_v2$entropy_one_word_after_cs_point)
plot(input_R_v2$sent_type,input_R_v2$frequency_negative_ln_first_cs_word_trans)
plot(input_R_v2$sent_type,input_R_v2$bilingual_corpus_frequency_negative_log_first_cs_word_trans)
###POSTAGS###
#Get Unique postags
unique(input_R_v2$pos_tag_first_cs_word_trans)
#https://www.sketchengine.eu/chinese-penn-treebank-part-of-speech-tagset/
#convert all postags that are NR (proper noun), NN (common noun) or NT (temporal noun) to "Noun"
input_R_v2<- input_R_v2 %>%
mutate(postag_converted = case_when(input_R_v2$pos_tag_first_cs_word_trans == 'NR' ~ 'Noun',
input_R_v2$pos_tag_first_cs_word_trans == 'NN' ~ 'Noun',
input_R_v2$pos_tag_first_cs_word_trans == 'NT' ~ 'Noun',
input_R_v2$pos_tag_first_cs_word_trans == 'VE' ~ 'Verb',
input_R_v2$pos_tag_first_cs_word_trans == 'VV' ~ 'Verb',
TRUE ~ 'Other'))
myvars <- c("sent_type", "pos_tag_first_cs_word_trans", "postag_converted")
new_view<-input_R_v2[myvars]
View(new_view)
#review levels
input_R_v2$postag_converted<- as.factor(input_R_v2$postag_converted)
levels(input_R_v2$postag_converted)
levels(input_R_v2$postag_converted) <- c("Noun","Verb", "Other")
#Plot the distribution of postags
ggplot(input_R_v2, aes(pos_tag_first_cs_word_trans, ..count..)) + geom_bar(aes(fill = sent_type), position = "dodge")
ggplot(input_R_v2, aes(postag_converted, ..count..)) + geom_bar(aes(fill = sent_type), position = "dodge")
nrow(input_R_v2[which(input_R_v2$postag_converted=="Noun"),])
nrow(input_R_v2[which(input_R_v2$postag_converted=="Verb"),])
nrow(input_R_v2[which(input_R_v2$postag_converted=="Other"),])
###DEPENDENCY RELATIONS###
#Get unique dependencies
unique(input_R_v2$deprel_first_cs_word_trans)
ggplot(input_R_v2, aes(deprel_first_cs_word_trans, ..count..)) + geom_bar(aes(fill = sent_type), position = "dodge")
summary(input_R_v2$deprel_first_cs_word_trans)
#Remove the dependencies with less than 100 observations
input_R_v2$deprel_first_cs_word_trans[which(input_R_v2$deprel_first_cs_word_trans!='compound:nn' & input_R_v2$deprel_first_cs_word_trans!='nsubj' & input_R_v2$deprel_first_cs_word_trans!='dobj'  & input_R_v2$deprel_first_cs_word_trans!='root' &  input_R_v2$deprel_first_cs_word_trans!='dep' & input_R_v2$deprel_first_cs_word_trans!='amod')] <- "Other"
input_R_v2$deprel_first_cs_word_trans<- as.factor(input_R_v2$deprel_first_cs_word_trans)
levels(input_R_v2$deprel_first_cs_word_trans)<-c("compound:nn","nsubj","dobj","root","dep","amod","Other")
ggplot(input_R_v2, aes(deprel_first_cs_word_trans, ..count..)) + geom_bar(aes(fill = sent_type), position = "dodge")
###DEPENDENCY DISTANCE###
#only consider dep distance if the governor is to the left of the cs-point, 0 otherwise
input_R_v2$dependency_distance_left <- ifelse(input_R_v2$word_id > input_R_v2$governor_first_cs_word_trans & input_R_v2$governor_first_cs_word_trans>0 , input_R_v2$word_id - input_R_v2$governor_first_cs_word_trans, 0)
myvars_dep<- c("sent_type", "word_id", "governor_first_cs_word_trans","dependency_distance","dependency_distance_left")
new_view2<-input_R_v2[myvars_dep]
View(new_view2)
plot(input_R_v2$sent_type,input_R_v2$dependency_distance_left)
###IF WORD IS AT THE BEGINNING, PREVIOUS_WORD_IS_PUNCTUATION IS CONSIDERED TRUE
input_R_v2$if_previous_word_is_punctuation [is.na(input_R_v2$if_previous_word_is_punctuation)] <- TRUE
#DEVIATION FROM MEAN SURPRISAL #NOT USED
input_R_v2$deviation_from_mean_surprisal <-abs(input_R_v2$average_surprisal-input_R_v2$surprisal_first_cs_word_trans)
myvars <- c("surprisal_first_cs_word_trans", "average_surprisal", "deviation_from_mean_surprisal")
new_view<-input_R_v2[myvars]
View(new_view)
#DEVIATION FROM MEAN SURPRISAL #NOT USED
input_R_v2$deviation_from_mean_surprisal <-abs(input_R_v2$average_surprisal-input_R_v2$surprisal_first_cs_word_trans)
myvars <- c("surprisal_first_cs_word_trans", "average_surprisal", "deviation_from_mean_surprisal")
new_view<-input_R_v2[myvars]
View(new_view)
rm(input_R_v2$deviation_from_mean_surprisal)
input_R_v2 = subset(input_R_v2, select = -c(deviation_from_mean_surprisal))
View(input_R_v2)
#There shouldn't be any NAs in the frequencies
nrow(input_R_v2)
nrow(input_R_v2[!(is.na(input_R_v2$frequency_negative_ln_first_cs_word_trans)),])
#SENTENCE LENGTHS
h <-hist(input_R_v2$translation_sentence_length, xlim = c(0,40), breaks=50)
text(h$mids,h$counts,labels=h$counts, adj=c(0.5, -0.5))
axis(side=1, at=seq(0,43, 1), labels=seq(0,43,1))
h
min(input_R_v2$translation_sentence_length)#2
max(input_R_v2$translation_sentence_length)#43
mean(input_R_v2$translation_sentence_length)#11.09
#CS-POINTS
cspoint_h<-hist(input_R_v2$word_id, xlim = c(0,40), breaks=30)
text(cspoint_h$mids,cspoint_h$counts,labels=cspoint_h$counts, adj=c(0.5, -0.5))
axis(side=1, at=seq(0,30, 1), labels=seq(0,30,1))
max(input_R_v2$word_id)#30
min(input_R_v2$word_id)#1
mean(input_R_v2$word_id)#5.154472
lambda= 1/mean(input_R_v2$word_id)# 0.194
std.dv=sqrt(1/lambda^2)
range=seq(0,5.154472 + 5*std.dv,0.01)
y=dexp(range,lambda)
plot(range,y,type="l", ylim=c(0,max(y)+0.01))
######################
###STANDARDIZATION###
######################
#not used:
input_R_v2$average_surprisal<-scale(input_R_v2$average_surprisal,center = TRUE, scale=TRUE) #not significant
#not defined for sentence-initial CSs:
input_R_v2$surprisal_of_previous_word<-scale(input_R_v2$surprisal_of_previous_word,center=TRUE, scale=TRUE) #not significant
input_R_v2$deviation_from_lastword_surprisal<-scale(input_R_v2$deviation_from_lastword_surprisal,center = TRUE, scale = TRUE) #not significant... plus the notion of surprisal is related to the last word; and this is not defined for initial words
input_R_v2$bilingual_corpus_frequency_negative_log_first_cs_word_trans<-scale(input_R_v2$bilingual_corpus_frequency_negative_log_first_cs_word_trans,center=TRUE,scale=TRUE)
input_R_v2$bilingual_corpus_frequency_negative_log_first_cs_word_trans<-input_R_v2$bilingual_corpus_frequency_negative_log_first_cs_word_trans * 0.5
#used
input_R_v2$frequency_negative_ln_first_cs_word_trans <- scale(input_R_v2$frequency_negative_ln_first_cs_word_trans, center=TRUE, scale=TRUE)
input_R_v2$frequency_negative_ln_first_cs_word_trans <-input_R_v2$frequency_negative_ln_first_cs_word_trans *0.5
input_R_v2$surprisal_first_cs_word_trans <- scale(input_R_v2$surprisal_first_cs_word_trans, center=TRUE, scale=TRUE)
input_R_v2$surprisal_first_cs_word_trans <-input_R_v2$surprisal_first_cs_word_trans *0.5
input_R_v2$deviation_from_mean_surprisal<-scale(input_R_v2$deviation_from_mean_surprisal,center = TRUE, scale = TRUE)
input_R_v2$deviation_from_mean_surprisal<-input_R_v2$deviation_from_mean_surprisal *0.5
input_R_v2$entropy_at_cs_point<-scale(input_R_v2$entropy_at_cs_point,center = TRUE, scale = TRUE)
input_R_v2$entropy_at_cs_point<-input_R_v2$entropy_at_cs_point* 0.5
input_R_v2$entropy_one_word_after_cs_point<-scale(input_R_v2$entropy_one_word_after_cs_point,center = TRUE, scale = TRUE)
